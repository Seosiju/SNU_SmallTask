import streamlit as st 
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter 
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain_community.chat_models import ChatOllama 
from langchain_core.prompts import ChatPromptTemplate 
from langchain.chains.combine_documents import create_stuff_documents_chain 
from langchain.chains import create_retrieval_chain 
import os

# --- 1. ëª¨ë¸ ë° RAG ì²´ì¸ ì„¤ì • (ìºì‹± ì‚¬ìš©) ---
# @st.cache_resource ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ì„ ìºì‹±
@st.cache_resource
def get_rag_chain(pdf_path):
    # PDF ê²½ë¡œì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    loader = PyPDFLoader(file_path=pdf_path)
    docs = loader.load()
    
    # ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_docs = text_splitter.split_documents(docs)
    
    # ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    retriever = vectorstore.as_retriever()
    
    # LLMì„ ì„¤ì •í•©ë‹ˆë‹¤.
    llm = ChatOllama(model="gemma3:4b")
    
    # í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_template("""
    Answer the following question based only on the provided context.

    <context>
    {context}
    </context>

    Question: {input}
    """)
    
    # RAG ì²´ì¸ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    
    return retrieval_chain

# --- 2. Streamlit UI êµ¬ì„± ---
st.title("ğŸ¦™ Ollama RAG: gemma3:4b")
st.markdown("ê°•ì˜ PDF ë¬¸ì„œì˜ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

# ë¶„ì„í•  PDF íŒŒì¼ ê²½ë¡œë¥¼ ì—¬ê¸°ì— ì§€ì •í•©ë‹ˆë‹¤.
PDF_FILE_PATH = "/Users/snu.sim/git/RAG_test/The Ghost in the Machine.pdf"

# ë©”ì¸ í™”ë©´ êµ¬ì„±
if not os.path.exists(PDF_FILE_PATH):
    st.error(f"ì§€ì •ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_FILE_PATH}")
    st.info("ì½”ë“œì˜ PDF_FILE_PATH ë³€ìˆ˜ì— ì˜¬ë°”ë¥¸ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    try:
        # ì§€ì •ëœ ê²½ë¡œì˜ íŒŒì¼ë¡œ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
        rag_chain = get_rag_chain(PDF_FILE_PATH)

        # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ë€
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?")

        if question:
            # ë¡œë”© ìŠ¤í”¼ë„ˆì™€ í•¨ê»˜ RAG ì²´ì¸ ì‹¤í–‰
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                response = rag_chain.invoke({"input": question})
                
                # ê²°ê³¼ ì¶œë ¥
                st.write("### ğŸ¤– AI ë‹µë³€:")
                st.write(response["answer"])

                # ê·¼ê±° ë¬¸ì„œ(Context) ì¶œë ¥ (í™•ì¥/ì¶•ì†Œ ê°€ëŠ¥)
                with st.expander("RAG Context í™•ì¸í•˜ê¸°"):
                    for i, doc in enumerate(response["context"]):
                        st.markdown(f"**ë¬¸ì„œ #{i+1}**")
                        st.write(doc.page_content)
                        st.markdown("---")
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")